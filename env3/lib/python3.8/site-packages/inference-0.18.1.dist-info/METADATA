Metadata-Version: 2.1
Name: inference
Version: 0.18.1
Summary: With no prior knowledge of machine learning or device-specific deployment, you can deploy a computer vision model to a range of devices and environments using Roboflow Inference.
Home-page: https://github.com/roboflow/inference
Author: Roboflow
Author-email: help@roboflow.com
Classifier: Development Status :: 5 - Production/Stable
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Education
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3 :: Only
Classifier: Topic :: Software Development
Classifier: Topic :: Scientific/Engineering
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Scientific/Engineering :: Image Recognition
Classifier: Typing :: Typed
Classifier: Operating System :: OS Independent
Requires-Python: >=3.8,<3.12
Description-Content-Type: text/markdown
License-File: LICENSE
License-File: LICENSE.core
Requires-Dist: APScheduler<=3.10.1
Requires-Dist: cython<=3.0.0
Requires-Dist: python-dotenv<=2.0.0
Requires-Dist: fastapi<0.111,>=0.100
Requires-Dist: numpy<=1.26.4
Requires-Dist: opencv-python<=4.10.0.84,>=4.8.1.78
Requires-Dist: piexif<=1.1.3
Requires-Dist: pillow<11.0
Requires-Dist: prometheus-fastapi-instrumentator<=6.0.0
Requires-Dist: redis<6.0.0
Requires-Dist: requests>=2.26.0
Requires-Dist: rich<=13.5.2
Requires-Dist: supervision<=0.22.0,>=0.20.0
Requires-Dist: pybase64<2.0.0
Requires-Dist: scikit-image>=0.19.0
Requires-Dist: requests-toolbelt>=1.0.0
Requires-Dist: wheel>=0.38.1
Requires-Dist: setuptools<=72.1.0,>=70.0.0
Requires-Dist: pytest-asyncio<=0.21.1
Requires-Dist: networkx>=3.1
Requires-Dist: pydantic~=2.6
Requires-Dist: pydantic-settings~=2.2
Requires-Dist: openai>=1.12.0
Requires-Dist: structlog>=24.1.0
Requires-Dist: zxing-cpp>=2.2.0
Requires-Dist: boto3<=1.34.123
Requires-Dist: typing-extensions>=4.8.0
Requires-Dist: pydot>=2.0.0
Requires-Dist: shapely<2.1.0,>=2.0.0
Requires-Dist: tldextract~=5.1.2
Requires-Dist: packaging~=24.0
Requires-Dist: anthropic~=0.34.2
Requires-Dist: onnxruntime<=1.15.1
Requires-Dist: GPUtil==1.4.0
Requires-Dist: requests<=2.31.0
Requires-Dist: docker==6.1.3
Requires-Dist: typer==0.9.0
Requires-Dist: skypilot[aws,gcp]==0.5.0
Requires-Dist: PyYAML>=6.0.0
Requires-Dist: supervision<1.0.0,>=0.20.0
Requires-Dist: tqdm>=4.0.0
Requires-Dist: GPUtil>=1.4.0
Requires-Dist: py-cpuinfo>=9.0.0
Requires-Dist: aiohttp>=3.9.0
Requires-Dist: backoff>=2.2.0
Requires-Dist: requests>=2.0.0
Requires-Dist: dataclasses-json>=0.6.0
Requires-Dist: pillow>=9.0.0
Requires-Dist: requests>=2.27.0
Requires-Dist: aioresponses>=0.7.6
Provides-Extra: clip
Requires-Dist: rf-clip==1.1; extra == "clip"
Provides-Extra: gaze
Requires-Dist: mediapipe<0.11,>=0.9; extra == "gaze"
Provides-Extra: grounding-dino
Requires-Dist: transformers<=4.42.2,>=4.36.0; extra == "grounding-dino"
Requires-Dist: rf-groundingdino==0.2.0; extra == "grounding-dino"
Provides-Extra: hosted
Requires-Dist: pymemcache<=4.0.0; extra == "hosted"
Requires-Dist: elasticache-auto-discovery<=1.0.0; extra == "hosted"
Requires-Dist: prometheus-fastapi-instrumentator<=6.0.0; extra == "hosted"
Provides-Extra: http
Requires-Dist: uvicorn[standard]<=0.22.0; extra == "http"
Requires-Dist: python-multipart<=0.0.9,>=0.0.7; extra == "http"
Requires-Dist: fastapi-cprofile<=0.0.2; extra == "http"
Requires-Dist: orjson>=3.9.10; extra == "http"
Requires-Dist: asgi-correlation-id>=4.3.1; extra == "http"
Provides-Extra: sam
Requires-Dist: rf-segment-anything==1.0; extra == "sam"
Requires-Dist: samv2==0.0.4; extra == "sam"
Requires-Dist: rasterio~=1.3; extra == "sam"
Requires-Dist: torch<=2.4.0,>=2.0.1; extra == "sam"
Requires-Dist: torchvision>=0.15.2; extra == "sam"
Provides-Extra: waf
Requires-Dist: metlo; extra == "waf"
Provides-Extra: yolo-world
Requires-Dist: ultralytics>=8.1.27; extra == "yolo-world"
Requires-Dist: dill==0.3.8; extra == "yolo-world"
Requires-Dist: rf-clip==1.1; extra == "yolo-world"

<div align="center">
  <p>
    <a align="center" href="" target="https://inference.roboflow.com/">
      <img
        width="100%"
        src="https://github.com/roboflow/inference/blob/main/banner.png?raw=true"
      >
    </a>
  </p>

  <br>

[notebooks](https://github.com/roboflow/notebooks) | [supervision](https://github.com/roboflow/supervision) | [autodistill](https://github.com/autodistill/autodistill) | [maestro](https://github.com/roboflow/multimodal-maestro)

  <br>

[![version](https://badge.fury.io/py/inference.svg)](https://badge.fury.io/py/inference)
[![downloads](https://img.shields.io/pypi/dm/inference)](https://pypistats.org/packages/inference)
![docker pulls](https://img.shields.io/docker/pulls/roboflow/roboflow-inference-server-cpu)
[![license](https://img.shields.io/pypi/l/inference)](https://github.com/roboflow/inference/blob/main/LICENSE.md)
[![huggingface](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/Roboflow/workflows)
[![discord](https://img.shields.io/discord/1159501506232451173)](https://discord.gg/GbfgXGJ8Bk)

</div>

## üëã hello

Roboflow Inference is an open-source platform designed to simplify the deployment of computer vision models. It enables developers to perform object detection, classification, and instance segmentation and utilize foundation models like [CLIP](https://inference.roboflow.com/foundation/clip), [Segment Anything](https://inference.roboflow.com/foundation/sam), and [YOLO-World](https://inference.roboflow.com/foundation/yolo_world) through a Python-native package, a self-hosted inference server, or a fully [managed API](https://docs.roboflow.com/).

Explore our [enterprise options](https://roboflow.com/sales) for advanced features like server deployment, device management, active learning, and commercial licenses for YOLOv5 and YOLOv8.

## üíª install

Inference package requires [**Python>=3.8,<=3.11**](https://www.python.org/). Click [here](https://inference.roboflow.com/quickstart/docker/) to learn more about running Inference inside Docker.

```bash
pip install inference
```

<details>
<summary>üëâ additional considerations</summary>


- hardware

  Enhance model performance in GPU-accelerated environments by installing CUDA-compatible dependencies.
  
  ```bash
  pip install inference-gpu
  ```

- models

  The `inference` and `inference-gpu` packages install only the minimal shared dependencies. Install model-specific dependencies to ensure code compatibility and license compliance. Learn more about the [models](https://inference.roboflow.com/#extras) supported by Inference.

  ```bash
  pip install inference[yolo-world]
  ```

</details>

## üî• quickstart

Use Inference SDK to run models locally with just a few lines of code. The image input can be a URL, a numpy array (BGR), or a PIL image.

```python
from inference import get_model

model = get_model(model_id="yolov8n-640")

results = model.infer("https://media.roboflow.com/inference/people-walking.jpg")
```

<details>
<summary>üëâ roboflow models</summary>

<br>

Set up your `ROBOFLOW_API_KEY` to access thousands of fine-tuned models shared by the [Roboflow Universe](https://universe.roboflow.com/) community and your custom model. Navigate to üîë keys section to learn more.

```python
from inference import get_model

model = get_model(model_id="soccer-players-5fuqs/1")

results = model.infer(
    image="https://media.roboflow.com/inference/soccer.jpg",
    confidence=0.5,
    iou_threshold=0.5
)
```

</details>

<details>
<summary>üëâ foundational models</summary>


- [CLIP Embeddings](https://inference.roboflow.com/foundation/clip) - generate text and image embeddings that you can use for zero-shot classification or assessing image similarity.

  ```python
  from inference.models import Clip

  model = Clip()

  embeddings_text = clip.embed_text("a football match")
  embeddings_image = model.embed_image("https://media.roboflow.com/inference/soccer.jpg")
  ```

- [Segment Anything](https://inference.roboflow.com/foundation/sam) - segment all objects visible in the image or only those associated with selected points or boxes.

  ```python
  from inference.models import SegmentAnything

  model = SegmentAnything()

  result = model.segment_image("https://media.roboflow.com/inference/soccer.jpg")
  ```

- [YOLO-World](https://inference.roboflow.com/foundation/yolo_world) - an almost real-time zero-shot detector that enables the detection of any objects without any training.

  ```python
  from inference.models import YOLOWorld

  model = YOLOWorld(model_id="yolo_world/l")
  
  result = model.infer(
      image="https://media.roboflow.com/inference/dog.jpeg",
      text=["person", "backpack", "dog", "eye", "nose", "ear", "tongue"],
      confidence=0.03
  )
  ```

</details>

## üìü inference server

- deploy server

  
  The inference server is distributed via Docker. Behind the scenes, inference will download and run the image that is appropriate for your hardware. [Here](https://inference.roboflow.com/quickstart/docker/#advanced-build-a-docker-container-from-scratch), you can learn more about the supported images.

  ```bash
  inference server start
  ```

- run client
  
  Consume inference server predictions using the HTTP client available in the Inference SDK.

  ```python
  from inference_sdk import InferenceHTTPClient
  
  client = InferenceHTTPClient(
      api_url="http://localhost:9001",
      api_key=<ROBOFLOW_API_KEY>
  )
  with client.use_model(model_id="soccer-players-5fuqs/1"):
      predictions = client.infer("https://media.roboflow.com/inference/soccer.jpg")
  ```
  
  If you're using the hosted API, change the local API URL to `https://detect.roboflow.com`. Accessing the hosted inference server and/or using any of the fine-tuned models require a `ROBOFLOW_API_KEY`. For further information, visit the üîë keys section.

## üé• inference pipeline

The inference pipeline is an efficient method for processing static video files and streams. Select a model, define the video source, and set a callback action. You can choose from predefined callbacks that allow you to [display results](https://inference.roboflow.com/docs/reference/inference/core/interfaces/stream/sinks/#inference.core.interfaces.stream.sinks.render_boxes) on the screen or [save them to a file](https://inference.roboflow.com/docs/reference/inference/core/interfaces/stream/sinks/#inference.core.interfaces.stream.sinks.VideoFileSink).

```python
from inference import InferencePipeline
from inference.core.interfaces.stream.sinks import render_boxes

pipeline = InferencePipeline.init(
    model_id="yolov8x-1280",
    video_reference="https://media.roboflow.com/inference/people-walking.mp4",
    on_prediction=render_boxes
)

pipeline.start()
pipeline.join()
```

## üîë keys

Inference enables the deployment of a wide range of pre-trained and foundational models without an API key. To access thousands of fine-tuned models shared by the [Roboflow Universe](https://universe.roboflow.com/) community, [configure your](https://app.roboflow.com/settings/api) API key.

```bash
export ROBOFLOW_API_KEY=<YOUR_API_KEY>
```

## üìö documentation

Visit our [documentation](https://inference.roboflow.com) to explore comprehensive guides, detailed API references, and a wide array of tutorials designed to help you harness the full potential of the Inference package.

## ‚ö°Ô∏è Model-specific extras

Explore the list of [`inference` extras](https://inference.roboflow.com/#extras) to install model-specific dependencies. 

## ¬© license

See the "Self Hosting and Edge Deployment" section of the [Roboflow Licensing](https://roboflow.com/licensing) documentation for information on how Roboflow Inference is licensed.

## üèÜ contribution

We would love your input to improve Roboflow Inference! Please see our [contributing guide](https://github.com/roboflow/inference/blob/master/CONTRIBUTING.md) to get started. Thank you to all of our contributors! üôè


<br>

<div align="center">
  <div align="center">
      <a href="https://youtube.com/roboflow">
          <img
            src="https://media.roboflow.com/notebooks/template/icons/purple/youtube.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949634652"
            width="3%"
          />
      </a>
      <img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%"/>
      <a href="https://roboflow.com">
          <img
            src="https://media.roboflow.com/notebooks/template/icons/purple/roboflow-app.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949746649"
            width="3%"
          />
      </a>
      <img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%"/>
      <a href="https://www.linkedin.com/company/roboflow-ai/">
          <img
            src="https://media.roboflow.com/notebooks/template/icons/purple/linkedin.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949633691"
            width="3%"
          />
      </a>
      <img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%"/>
      <a href="https://docs.roboflow.com">
          <img
            src="https://media.roboflow.com/notebooks/template/icons/purple/knowledge.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949634511"
            width="3%"
          />
      </a>
      <img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%"/>
      <a href="https://disuss.roboflow.com">
          <img
            src="https://media.roboflow.com/notebooks/template/icons/purple/forum.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949633584"
            width="3%"
          />
      <img src="https://raw.githubusercontent.com/ultralytics/assets/main/social/logo-transparent.png" width="3%"/>
      <a href="https://blog.roboflow.com">
          <img
            src="https://media.roboflow.com/notebooks/template/icons/purple/blog.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672949633605"
            width="3%"
          />
      </a>
      </a>
  </div>
</div>
